{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cats vs. Dogs ##\n",
    "In this project our objective will be distinguishing between images of cats and images of dogs.\n",
    "\n",
    "Kaggle link: https://www.kaggle.com/competitions/dogs-vs-cats/overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the Data ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import KMeans\n",
    "import os, cv2\n",
    "import numpy as np\n",
    "\n",
    "# dog = 0, cat = 1\n",
    "def LoadAnimals(directory, clustering=0):\n",
    "    x = []\n",
    "    y = []\n",
    "\n",
    "    for filename in os.listdir(directory):\n",
    "        if 'dog' in filename:\n",
    "            animal_class = 0\n",
    "        else: # cat\n",
    "            animal_class = 1\n",
    "\n",
    "        f = os.path.join(directory, filename)\n",
    "        image = cv2.imread(f, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        if clustering != 0:\n",
    "            reshaped_image = image.reshape(-1, 3)\n",
    "            kmeans = KMeans(n_clusters=clustering).fit(reshaped_image)\n",
    "            segmented_img = kmeans.cluster_centers_[kmeans.labels_]\n",
    "            segmented_img = segmented_img.reshape(image.shape)\n",
    "            image = segmented_img\n",
    "\n",
    "        x.append(np.concatenate(image)) # sklearn doesn't like 3d arrays (x_train is an array of 2d arrays -> 3d array). Transforms 3d array to 2d.\n",
    "        y.append(animal_class)\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = LoadAnimals(r\"C:\\Users\\Daniel\\Pictures\\resize100,100\", clustering=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA Preparation ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>9990</th>\n",
       "      <th>9991</th>\n",
       "      <th>9992</th>\n",
       "      <th>9993</th>\n",
       "      <th>9994</th>\n",
       "      <th>9995</th>\n",
       "      <th>9996</th>\n",
       "      <th>9997</th>\n",
       "      <th>9998</th>\n",
       "      <th>9999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>168</td>\n",
       "      <td>173</td>\n",
       "      <td>177</td>\n",
       "      <td>180</td>\n",
       "      <td>185</td>\n",
       "      <td>188</td>\n",
       "      <td>190</td>\n",
       "      <td>193</td>\n",
       "      <td>197</td>\n",
       "      <td>198</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>43</td>\n",
       "      <td>43</td>\n",
       "      <td>44</td>\n",
       "      <td>43</td>\n",
       "      <td>52</td>\n",
       "      <td>32</td>\n",
       "      <td>52</td>\n",
       "      <td>37</td>\n",
       "      <td>40</td>\n",
       "      <td>61</td>\n",
       "      <td>...</td>\n",
       "      <td>47</td>\n",
       "      <td>50</td>\n",
       "      <td>48</td>\n",
       "      <td>50</td>\n",
       "      <td>55</td>\n",
       "      <td>61</td>\n",
       "      <td>63</td>\n",
       "      <td>89</td>\n",
       "      <td>18</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>39</td>\n",
       "      <td>25</td>\n",
       "      <td>42</td>\n",
       "      <td>52</td>\n",
       "      <td>45</td>\n",
       "      <td>55</td>\n",
       "      <td>61</td>\n",
       "      <td>55</td>\n",
       "      <td>54</td>\n",
       "      <td>58</td>\n",
       "      <td>...</td>\n",
       "      <td>157</td>\n",
       "      <td>157</td>\n",
       "      <td>149</td>\n",
       "      <td>134</td>\n",
       "      <td>129</td>\n",
       "      <td>149</td>\n",
       "      <td>159</td>\n",
       "      <td>168</td>\n",
       "      <td>171</td>\n",
       "      <td>153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>220</td>\n",
       "      <td>224</td>\n",
       "      <td>223</td>\n",
       "      <td>227</td>\n",
       "      <td>229</td>\n",
       "      <td>223</td>\n",
       "      <td>226</td>\n",
       "      <td>235</td>\n",
       "      <td>229</td>\n",
       "      <td>226</td>\n",
       "      <td>...</td>\n",
       "      <td>229</td>\n",
       "      <td>213</td>\n",
       "      <td>206</td>\n",
       "      <td>204</td>\n",
       "      <td>202</td>\n",
       "      <td>214</td>\n",
       "      <td>222</td>\n",
       "      <td>215</td>\n",
       "      <td>212</td>\n",
       "      <td>213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>117</td>\n",
       "      <td>117</td>\n",
       "      <td>122</td>\n",
       "      <td>92</td>\n",
       "      <td>78</td>\n",
       "      <td>97</td>\n",
       "      <td>134</td>\n",
       "      <td>130</td>\n",
       "      <td>150</td>\n",
       "      <td>180</td>\n",
       "      <td>...</td>\n",
       "      <td>95</td>\n",
       "      <td>95</td>\n",
       "      <td>95</td>\n",
       "      <td>91</td>\n",
       "      <td>101</td>\n",
       "      <td>121</td>\n",
       "      <td>96</td>\n",
       "      <td>90</td>\n",
       "      <td>93</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24995</th>\n",
       "      <td>140</td>\n",
       "      <td>142</td>\n",
       "      <td>143</td>\n",
       "      <td>145</td>\n",
       "      <td>146</td>\n",
       "      <td>147</td>\n",
       "      <td>148</td>\n",
       "      <td>149</td>\n",
       "      <td>148</td>\n",
       "      <td>149</td>\n",
       "      <td>...</td>\n",
       "      <td>164</td>\n",
       "      <td>165</td>\n",
       "      <td>166</td>\n",
       "      <td>166</td>\n",
       "      <td>167</td>\n",
       "      <td>168</td>\n",
       "      <td>168</td>\n",
       "      <td>168</td>\n",
       "      <td>168</td>\n",
       "      <td>169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24996</th>\n",
       "      <td>119</td>\n",
       "      <td>77</td>\n",
       "      <td>103</td>\n",
       "      <td>84</td>\n",
       "      <td>99</td>\n",
       "      <td>114</td>\n",
       "      <td>71</td>\n",
       "      <td>82</td>\n",
       "      <td>85</td>\n",
       "      <td>87</td>\n",
       "      <td>...</td>\n",
       "      <td>95</td>\n",
       "      <td>94</td>\n",
       "      <td>80</td>\n",
       "      <td>98</td>\n",
       "      <td>86</td>\n",
       "      <td>77</td>\n",
       "      <td>85</td>\n",
       "      <td>94</td>\n",
       "      <td>92</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24997</th>\n",
       "      <td>42</td>\n",
       "      <td>35</td>\n",
       "      <td>47</td>\n",
       "      <td>56</td>\n",
       "      <td>66</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>57</td>\n",
       "      <td>59</td>\n",
       "      <td>39</td>\n",
       "      <td>...</td>\n",
       "      <td>102</td>\n",
       "      <td>135</td>\n",
       "      <td>135</td>\n",
       "      <td>123</td>\n",
       "      <td>71</td>\n",
       "      <td>116</td>\n",
       "      <td>180</td>\n",
       "      <td>127</td>\n",
       "      <td>114</td>\n",
       "      <td>167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24998</th>\n",
       "      <td>253</td>\n",
       "      <td>250</td>\n",
       "      <td>246</td>\n",
       "      <td>234</td>\n",
       "      <td>223</td>\n",
       "      <td>208</td>\n",
       "      <td>187</td>\n",
       "      <td>145</td>\n",
       "      <td>114</td>\n",
       "      <td>73</td>\n",
       "      <td>...</td>\n",
       "      <td>120</td>\n",
       "      <td>101</td>\n",
       "      <td>106</td>\n",
       "      <td>112</td>\n",
       "      <td>112</td>\n",
       "      <td>112</td>\n",
       "      <td>106</td>\n",
       "      <td>118</td>\n",
       "      <td>115</td>\n",
       "      <td>119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24999</th>\n",
       "      <td>239</td>\n",
       "      <td>241</td>\n",
       "      <td>243</td>\n",
       "      <td>242</td>\n",
       "      <td>241</td>\n",
       "      <td>243</td>\n",
       "      <td>247</td>\n",
       "      <td>251</td>\n",
       "      <td>246</td>\n",
       "      <td>244</td>\n",
       "      <td>...</td>\n",
       "      <td>254</td>\n",
       "      <td>254</td>\n",
       "      <td>254</td>\n",
       "      <td>254</td>\n",
       "      <td>254</td>\n",
       "      <td>254</td>\n",
       "      <td>254</td>\n",
       "      <td>254</td>\n",
       "      <td>254</td>\n",
       "      <td>254</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25000 rows × 10000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0     1     2     3     4     5     6     7     8     9     ...  9990  \\\n",
       "0       168   173   177   180   185   188   190   193   197   198  ...     3   \n",
       "1        43    43    44    43    52    32    52    37    40    61  ...    47   \n",
       "2        39    25    42    52    45    55    61    55    54    58  ...   157   \n",
       "3       220   224   223   227   229   223   226   235   229   226  ...   229   \n",
       "4       117   117   122    92    78    97   134   130   150   180  ...    95   \n",
       "...     ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  ...   ...   \n",
       "24995   140   142   143   145   146   147   148   149   148   149  ...   164   \n",
       "24996   119    77   103    84    99   114    71    82    85    87  ...    95   \n",
       "24997    42    35    47    56    66    48    48    57    59    39  ...   102   \n",
       "24998   253   250   246   234   223   208   187   145   114    73  ...   120   \n",
       "24999   239   241   243   242   241   243   247   251   246   244  ...   254   \n",
       "\n",
       "       9991  9992  9993  9994  9995  9996  9997  9998  9999  \n",
       "0         3     3     2     2     2     2     2     2     2  \n",
       "1        50    48    50    55    61    63    89    18    35  \n",
       "2       157   149   134   129   149   159   168   171   153  \n",
       "3       213   206   204   202   214   222   215   212   213  \n",
       "4        95    95    91   101   121    96    90    93    81  \n",
       "...     ...   ...   ...   ...   ...   ...   ...   ...   ...  \n",
       "24995   165   166   166   167   168   168   168   168   169  \n",
       "24996    94    80    98    86    77    85    94    92    76  \n",
       "24997   135   135   123    71   116   180   127   114   167  \n",
       "24998   101   106   112   112   112   106   118   115   119  \n",
       "24999   254   254   254   254   254   254   254   254   254  \n",
       "\n",
       "[25000 rows x 10000 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(np.array(x))\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll make a graph that will help us understand which n_components is the most optimal for us and why"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pca_10000 = PCA(n_components=10000)\n",
    "pca_10000.fit(df)\n",
    "\n",
    "plt.grid()\n",
    "plt.plot(np.cumsum(pca_10000.explained_variance_ratio_ * 1000))\n",
    "plt.xlabel('Number of components')\n",
    "plt.ylabel('Explained variance')\n",
    "plt.savefig('Scree plot.png') \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the graph, it seems that n_components = 1000 is the best fit in our opinion. It keeps about 94% of the variance, hence we only lose a small amount of data, in exchange to increasing the time efficiency by alot. That is since we reduced 9000 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(n_components=1000)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca_1000 = PCA(n_components=1000)\n",
    "pca_1000.fit(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll calculate the exact explained variance for n=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94.30864613023468"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.cumsum(pca_1000.explained_variance_ratio_ * 100)[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll use the new data with reduced dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_x = pca_1000.fit_transform(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing Feature Importance ##\n",
    "Using Random Forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def array1dto2d(arr, w, h):\n",
    "    if w*h != len(arr):\n",
    "        return None\n",
    "    else:\n",
    "        ret = []\n",
    "        for i in range(0, h):\n",
    "            ret.append( arr[w*i:w*(i+1)] )\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "rnd_clf = RandomForestClassifier(n_estimators=500, n_jobs=-1)\n",
    "rnd_clf.fit(x, y)\n",
    "\n",
    "impprtances = array1dto2d(rnd_clf.feature_importances_, 100, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(impprtances, vmin=0, vmax=0.005, cmap=ListedColormap(['black', 'yellow', 'orange', 'red']))\n",
    "plt.xlim(0, 100)\n",
    "plt.ylim(0, 100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: It seems our pictures don't have a pattern and a specific area in which the important (or non-important, a dead space such the top right corner commonly being a wall) information is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def Test(name, model, new_x=x, new_y=y, new_cv=30, print_flag=True, return_flag=False):\n",
    "    score = abs(np.median( cross_val_score(model, new_x, new_y, cv=new_cv, scoring='accuracy') ))\n",
    "    if print_flag:\n",
    "        print('{}: {}'.format(name, score))\n",
    "    if return_flag:\n",
    "        return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will run a dummy to set our baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dummy (stratified): 0.5056\n",
      "Dummy (most_frequent): 0.5\n"
     ]
    }
   ],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "Test('Dummy (stratified)', DummyClassifier(strategy=\"stratified\"))\n",
    "Test('Dummy (most_frequent)', DummyClassifier(strategy=\"most_frequent\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will try multiple models, to increase our accuracy using Voting classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Daniel\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Daniel\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Daniel\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Daniel\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Daniel\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Daniel\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Daniel\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Daniel\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Daniel\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression: 0.5458000000000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Daniel\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "Test('Logistic Regression', LogisticRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest: 0.5932\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "Test('Random Forest', RandomForestClassifier())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets try some new models: AdaBoost, XGBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Daniel\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:56:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Daniel\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:58:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Daniel\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:59:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Daniel\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:01:37] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Daniel\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:03:21] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Daniel\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:05:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Daniel\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:06:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Daniel\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:08:41] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Daniel\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:10:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Daniel\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:12:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBoost: 0.625\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "Test('XGBoost', XGBClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost: 0.6118\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "Test('AdaBoost', AdaBoostClassifier())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding optimal K for KNN by trying values of K:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Daniel\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "30 fits failed out of a total of 30.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "30 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Daniel\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Daniel\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py\", line 198, in fit\n",
      "    return self._fit(X, y)\n",
      "  File \"C:\\Users\\Daniel\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_base.py\", line 569, in _fit\n",
      "    raise ValueError(\"Expected n_neighbors > 0. Got %d\" % self.n_neighbors)\n",
      "ValueError: Expected n_neighbors > 0. Got 0\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN (best k 17): 0.6038415366146459\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "best_score = (0,0)\n",
    "for k in range(0, 21):\n",
    "    score = Test('KNN', KNeighborsClassifier(n_neighbors=k), new_x=transformed_x, print_flag=False, return_flag=True)\n",
    "    if score > best_score[1]:\n",
    "        best_score = (k, score)\n",
    "\n",
    "print('KNN (best k {}): {}'.format(best_score[0], best_score[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC: 0.6608\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "Test('SVC', SVC())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tried running Bagging with Random Forest, and it was not successful. 1% Boost in accuracy, however it is 4 times longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging Classifier: 0.6024\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "bagging = BaggingClassifier(RandomForestClassifier(), n_estimators=10, max_samples=12500)\n",
    "Test('Bagging Classifier', bagging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianNB: 0.5684\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "Test('GaussianNB', GaussianNB())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final results are:\n",
    "\n",
    "\n",
    "LogisticRegression: 55\n",
    "\n",
    "RandomForestClassifier: 59\n",
    "\n",
    "XGBClassifier: 63\n",
    "\n",
    "Adaboost: 61\n",
    "\n",
    "KNN(k=17): 60\n",
    "\n",
    "SVC: 66\n",
    "\n",
    "GaussianNB: 57"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided to run Hard Voting on all the models that have 60% or more accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Daniel\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:22:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "estimators = []\n",
    "model3 = SVC(); estimators.append((\"svc\", model3))\n",
    "model4 = XGBClassifier(); estimators.append((\"XGB\",model4))\n",
    "model5 = AdaBoostClassifier(); estimators.append((\"AdaB\",model5))\n",
    "model6 = KNeighborsClassifier(n_neighbors=17); estimators.append((\"KNN\",model6))\n",
    "\n",
    "ensemble = VotingClassifier(estimators)\n",
    "\n",
    "Test(\"HardVoting\",ensemble)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f2b6fda8d17b1a5f6c1f9b2bd60732f0f07346c1624e69138d7c670971a5b21a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
